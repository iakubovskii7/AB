{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Алгоритм Томпсона на батчах (без контекста)\n",
    "\n",
    "1. На первом батче распределяем юзеров 50% на 50%.\n",
    "2. Вероятность конверсии каждого варианта распределена по Beta распределению с $\\alpha=1$,\n",
    "$\\beta=1$.\n",
    "3. В конце каждого батча пересчитываем вероятности превосходства по точной формуле,\n",
    "взятой отсюда https://www.johndcook.com//UTMDABTR-005-05.pdf\n",
    "4. Распределяем трафик в пропорции вероятностей превосходства для каждого варианта\n",
    "5. Останавливаем эксперимент при достижении определенной вероятности превосходства,\n",
    "но не раньше определенного дня, чтобы учесть календарные факторы\n",
    "\n",
    "Потенциальные проблемы:\n",
    "- слишком рано отдаем трафик победителю\n",
    "- из-за дисбаланса распределения трафика может быть больше успешных конверсий в этом варианте\n",
    "(можно попробовать применить нормализацию)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Реализуем алгоритм***\n",
    "\n",
    "0. **Инициализация** - *BatchThompson(n_arms)*. Аргумент на вход: число вариантов сплита.\n",
    "Здесь также инициализируются массивы для параметров Бета-распределений и вероятность превосходства = 0.5.\n",
    "\n",
    "1. **Метод сплита** - *split_data()*. Исходя из вероятности превосходства вычисляем сплит по вариантам.\n",
    "Возвращаем данные по конверсии на текущем батче для пересчета Бета-распределений.\n",
    "\n",
    "2. **Метод изменения параметров распределения** - *.update_beta_params(data)*. Аргументы на вход: numpy массив со значениями конверсии по\n",
    "каждому варианту. В случае неравномерного распределения по вариантам ставятся пропуски.\n",
    " - Проверяем, чтобы число столбцов совпадало с числом вариантов из инициализации.\n",
    " - Суммируем нули и единицы и обновляем параметры\n",
    "\n",
    "3. **Метод пересчета** - *update_prob_super()*. Аргументов нет, так как учитывает измененные параметры\n",
    "$\\alpha$ (накопленное число успешных конверсий) и\n",
    "$\\beta$ (накопленное число неудачных конверсий) для всех вариантов.\n",
    " - Считаем по точной формуле\n",
    " - Выдаем массив из вероятностей превосходства\n",
    "\n",
    "4. **Вероятность превосходства** - *prob_super_tuple()*. Аргументов нет, так как берем пересчитанные параметры.\n",
    "5. **Критерий остановки** (*stopping_criterion*) - условия цикла while. Либо вероятность превосходства выше заданной\n",
    "величины, либо закончились наблюдения."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import pandas as pd\n",
    "from numpy import ndarray\n",
    "from tqdm.notebook import tqdm\n",
    "from src.ab import get_size_zratio\n",
    "from src.mab import calc_prob_between, expected_loss,\n",
    "\n",
    "# Графики\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from src.mab import BatchThompson, BatchThompsonMixed\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from joblib import Parallel, delayed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Эксперименты на разных теоретических конверсиях и размерах батча (summation vs normalization)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем реализовать метод, описанный в статье\n",
    "https://www.researchgate.net/publication/352117401_Parallelizing_Thompson_Sampling\n",
    "\n",
    "Авторы предлагают следующий алгоритм. Пусть $k_a$ - число раз выбора руки $a$,\n",
    "$l_a = 1$ - число раз выбора руки подряд.\n",
    "\n",
    "Для каждого батча $t = 1, 2, .. T$:\n",
    "\n",
    "- смотрим на вероятности бета распределений\n",
    "- выбираем руку с наибольшей вероятностью\n",
    "- присваиваем $k_a = k_a + 1$\n",
    "- ЕСЛИ $k_a < 2^{l_a}$, то кидаем ВЕСЬ трафик в эту руку\n",
    "- ИНАЧЕ: присваиваем $l_a = l_a + 1$ и распределяем трафик в ОБЕ руки ПОЛНОСТЬЮ (без долей)\n",
    "- обновляем параметры и по новой\n",
    "\n",
    "Утверждается, что он довольно хорошо работает и для динамических батчей - когда размер заранее нам неизвестен\n",
    "Как выяснилось - работает не очень. Поэтому объединим два метода в один."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc230f054cd547bc88c64f05cb99a717"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.15 0.012\n",
      "0.044 0.15 0.045\n",
      "0.044 0.15 0.056\n",
      "0.061 0.15 0.045\n",
      "0.061 0.15 0.056\n",
      "0.078 0.15 0.001\n",
      "0.078 0.15 0.012\n",
      "0.145 0.15 0.023\n",
      "0.162 0.01 0.034\n",
      "0.162 0.01 0.1\n",
      "0.162 0.15 0.012\n",
      "0.179 0.01 0.1\n",
      "0.179 0.15 0.045\n",
      "0.179 0.15 0.067\n",
      "0.179 0.15 0.089\n",
      "0.196 0.01 0.023\n",
      "0.196 0.15 0.045\n",
      "0.196 0.15 0.078\n",
      "0.196 0.15 0.1\n",
      "0.213 0.01 0.045\n",
      "0.213 0.01 0.078\n",
      "0.213 0.15 0.023\n",
      "0.213 0.15 0.056\n",
      "0.213 0.15 0.078\n",
      "0.213 0.15 0.1\n",
      "0.23 0.01 0.012\n",
      "0.23 0.01 0.089\n",
      "0.23 0.15 0.001\n",
      "0.331 0.01 0.034\n",
      "0.331 0.01 0.045\n",
      "0.331 0.01 0.078\n",
      "0.331 0.15 0.001\n",
      "0.416 0.15 0.045\n",
      "0.416 0.15 0.067\n",
      "0.416 0.15 0.1\n",
      "0.432 0.01 0.023\n",
      "0.432 0.01 0.034\n",
      "0.432 0.01 0.045\n",
      "0.432 0.01 0.078\n",
      "0.432 0.15 0.001\n",
      "0.5 0.01 0.067\n",
      "0.5 0.15 0.012\n",
      "0.01 0.15 0.001\n",
      "0.128 0.15 0.012\n",
      "0.128 0.15 0.023\n",
      "0.128 0.15 0.034\n",
      "0.128 0.15 0.045\n",
      "0.128 0.15 0.056\n",
      "0.128 0.15 0.089\n",
      "0.145 0.01 0.001\n",
      "0.263 0.01 0.045\n",
      "0.263 0.01 0.067\n",
      "0.263 0.01 0.1\n",
      "0.263 0.15 0.012\n",
      "0.28 0.01 0.012\n",
      "0.28 0.01 0.1\n",
      "0.28 0.15 0.034\n",
      "0.28 0.15 0.045\n",
      "0.28 0.15 0.067\n",
      "0.28 0.15 0.078\n",
      "0.297 0.01 0.001\n",
      "0.416 0.15 0.023\n",
      "0.416 0.15 0.078\n",
      "0.432 0.01 0.012\n",
      "0.432 0.01 0.056\n",
      "0.432 0.01 0.1\n",
      "0.432 0.15 0.034\n",
      "0.432 0.15 0.045\n",
      "0.432 0.15 0.067\n",
      "0.432 0.15 0.089\n",
      "0.449 0.01 0.001\n",
      "0.01 0.01 0.1\n",
      "0.044 0.01 0.023\n",
      "0.044 0.01 0.034\n",
      "0.061 0.15 0.067\n",
      "0.061 0.15 0.078\n",
      "0.078 0.01 0.089\n",
      "0.078 0.01 0.1\n",
      "0.078 0.15 0.067\n",
      "0.078 0.15 0.1\n",
      "0.094 0.01 0.023\n",
      "0.094 0.01 0.078\n",
      "0.094 0.15 0.012\n",
      "0.111 0.01 0.023\n",
      "0.111 0.01 0.056\n",
      "0.111 0.01 0.089\n",
      "0.111 0.01 0.1\n",
      "0.111 0.15 0.023\n",
      "0.111 0.15 0.034\n",
      "0.111 0.15 0.045\n",
      "0.111 0.15 0.078\n",
      "0.111 0.15 0.089\n",
      "0.128 0.01 0.001\n",
      "0.23 0.15 0.034\n",
      "0.23 0.15 0.045\n",
      "0.23 0.15 0.067\n",
      "0.23 0.15 0.1\n",
      "0.247 0.01 0.034\n",
      "0.247 0.01 0.045\n",
      "0.247 0.01 0.067\n",
      "0.247 0.01 0.089\n",
      "0.247 0.15 0.012\n",
      "0.247 0.15 0.056\n",
      "0.247 0.15 0.089\n",
      "0.247 0.15 0.1\n",
      "0.263 0.01 0.012\n",
      "0.263 0.01 0.023\n",
      "0.263 0.01 0.078\n",
      "0.263 0.15 0.001\n",
      "0.365 0.15 0.045\n",
      "0.365 0.15 0.067\n",
      "0.365 0.15 0.1\n",
      "0.382 0.01 0.023\n",
      "0.382 0.01 0.045\n",
      "0.382 0.01 0.078\n",
      "0.382 0.15 0.001\n",
      "0.466 0.01 0.034\n",
      "0.466 0.01 0.045\n",
      "0.466 0.01 0.067\n",
      "0.466 0.01 0.1\n",
      "0.466 0.15 0.023\n",
      "0.466 0.15 0.056\n",
      "0.466 0.15 0.078\n",
      "0.466 0.15 0.1\n",
      "0.483 0.01 0.012\n",
      "0.483 0.01 0.045\n",
      "0.483 0.01 0.1\n",
      "0.483 0.15 0.023\n",
      "0.483 0.15 0.045\n",
      "0.483 0.15 0.078\n",
      "0.5 0.01 0.012\n",
      "0.5 0.15 0.045\n",
      "0.5 0.15 0.067\n",
      "0.5 0.15 0.1\n",
      "0.01 0.15 0.034\n",
      "0.027 0.15 0.089\n",
      "0.027 0.15 0.1\n",
      "0.044 0.01 0.089\n",
      "0.044 0.01 0.1\n",
      "0.061 0.01 0.001\n",
      "0.061 0.01 0.012\n",
      "0.145 0.01 0.034\n",
      "0.145 0.01 0.045\n",
      "0.145 0.01 0.089\n",
      "0.145 0.15 0.012\n",
      "0.162 0.01 0.067\n",
      "0.162 0.15 0.034\n",
      "0.179 0.01 0.056\n",
      "0.179 0.01 0.067\n",
      "0.179 0.15 0.001\n",
      "0.28 0.01 0.001\n",
      "0.382 0.01 0.001\n",
      "0.483 0.15 0.034\n",
      "0.483 0.15 0.067\n",
      "0.5 0.01 0.001\n",
      "0.01 0.01 0.089\n",
      "0.027 0.15 0.012\n",
      "0.061 0.15 0.001\n",
      "0.061 0.15 0.012\n",
      "0.145 0.15 0.001\n",
      "0.263 0.01 0.034\n",
      "0.263 0.01 0.056\n",
      "0.263 0.01 0.089\n",
      "0.263 0.15 0.023\n",
      "0.263 0.15 0.045\n",
      "0.263 0.15 0.067\n",
      "0.263 0.15 0.089\n",
      "0.28 0.01 0.034\n",
      "0.28 0.01 0.045\n",
      "0.28 0.01 0.067\n",
      "0.28 0.01 0.089\n",
      "0.28 0.15 0.012\n",
      "0.297 0.01 0.023\n",
      "0.297 0.01 0.089\n",
      "0.297 0.15 0.023\n",
      "0.297 0.15 0.045\n",
      "0.297 0.15 0.067\n",
      "0.314 0.01 0.001\n",
      "0.399 0.15 0.001\n",
      "0.466 0.15 0.034\n",
      "0.466 0.15 0.045\n",
      "0.466 0.15 0.067\n",
      "0.466 0.15 0.089\n",
      "0.483 0.01 0.001\n",
      "0.01 0.01 0.034\n",
      "0.027 0.15 0.023\n",
      "0.027 0.15 0.034\n",
      "0.061 0.01 0.045\n",
      "0.061 0.01 0.056\n",
      "0.078 0.01 0.023\n",
      "0.078 0.01 0.034\n",
      "0.094 0.01 0.056\n",
      "0.094 0.01 0.1\n",
      "0.094 0.15 0.034\n",
      "0.094 0.15 0.056\n",
      "0.094 0.15 0.089\n",
      "0.111 0.01 0.034\n",
      "0.111 0.01 0.045\n",
      "0.111 0.01 0.067\n",
      "0.111 0.01 0.078\n",
      "0.111 0.15 0.001\n",
      "0.179 0.15 0.056\n",
      "0.179 0.15 0.1\n",
      "0.196 0.01 0.045\n",
      "0.196 0.01 0.089\n",
      "0.196 0.15 0.012\n",
      "0.213 0.01 0.023\n",
      "0.213 0.01 0.089\n",
      "0.213 0.15 0.012\n",
      "0.23 0.01 0.045\n",
      "0.23 0.01 0.067\n",
      "0.23 0.01 0.078\n",
      "0.23 0.15 0.012\n",
      "0.23 0.15 0.089\n",
      "0.247 0.01 0.023\n",
      "0.247 0.01 0.056\n",
      "0.247 0.01 0.078\n",
      "0.247 0.01 0.1\n",
      "0.247 0.15 0.023\n",
      "0.247 0.15 0.034\n",
      "0.247 0.15 0.045\n",
      "0.247 0.15 0.067\n",
      "0.247 0.15 0.078\n",
      "0.263 0.01 0.001\n",
      "0.382 0.15 0.034\n",
      "0.382 0.15 0.045\n",
      "0.382 0.15 0.067\n",
      "0.382 0.15 0.1\n",
      "0.399 0.01 0.012\n",
      "0.399 0.01 0.078\n",
      "0.399 0.01 0.1\n",
      "0.399 0.15 0.034\n",
      "0.399 0.15 0.045\n",
      "0.399 0.15 0.078\n",
      "0.399 0.15 0.1\n",
      "0.416 0.01 0.012\n",
      "0.416 0.01 0.067\n",
      "0.416 0.01 0.089\n",
      "0.416 0.15 0.001\n",
      "0.483 0.15 0.001\n",
      "0.01 0.15 0.045\n",
      "0.01 0.15 0.067\n",
      "0.027 0.01 0.001\n",
      "0.128 0.15 0.078\n",
      "0.145 0.01 0.012\n",
      "0.145 0.15 0.034\n",
      "0.162 0.01 0.001\n",
      "0.297 0.01 0.045\n",
      "0.297 0.01 0.067\n",
      "0.297 0.01 0.1\n",
      "0.297 0.15 0.012\n",
      "0.297 0.15 0.089\n",
      "0.314 0.01 0.012\n",
      "0.314 0.15 0.034\n",
      "0.314 0.15 0.056\n",
      "0.314 0.15 0.078\n",
      "0.331 0.01 0.001\n",
      "0.449 0.01 0.078\n",
      "0.449 0.15 0.034\n",
      "0.449 0.15 0.045\n",
      "0.449 0.15 0.078\n",
      "0.466 0.01 0.001\n",
      "0.01 0.01 0.078\n",
      "0.044 0.01 0.001\n",
      "0.044 0.01 0.012\n",
      "0.162 0.01 0.089\n",
      "0.162 0.15 0.045\n",
      "0.162 0.15 0.089\n",
      "0.179 0.01 0.023\n",
      "0.179 0.15 0.012\n",
      "0.196 0.01 0.056\n",
      "0.196 0.15 0.001\n",
      "0.297 0.01 0.012\n",
      "0.297 0.15 0.034\n",
      "0.297 0.15 0.056\n",
      "0.297 0.15 0.078\n",
      "0.297 0.15 0.1\n",
      "0.314 0.01 0.023\n",
      "0.314 0.01 0.089\n",
      "0.314 0.15 0.012\n",
      "0.314 0.15 0.1\n",
      "0.331 0.01 0.023\n",
      "0.331 0.01 0.067\n",
      "0.331 0.01 0.1\n",
      "0.331 0.15 0.023\n",
      "0.331 0.15 0.056\n",
      "0.331 0.15 0.067\n",
      "0.331 0.15 0.1\n",
      "0.348 0.01 0.023\n",
      "0.348 0.01 0.045\n",
      "0.348 0.01 0.089\n",
      "0.348 0.15 0.012\n",
      "0.365 0.01 0.023\n",
      "0.365 0.01 0.067\n",
      "0.365 0.01 0.089\n",
      "0.365 0.15 0.012\n",
      "0.365 0.15 0.078\n",
      "0.365 0.15 0.089\n",
      "0.382 0.01 0.012\n",
      "0.382 0.01 0.089\n",
      "0.382 0.15 0.012\n",
      "0.399 0.01 0.023\n",
      "0.399 0.01 0.045\n",
      "0.399 0.01 0.067\n",
      "0.399 0.15 0.023\n",
      "0.399 0.15 0.056\n",
      "0.399 0.15 0.067\n",
      "0.399 0.15 0.089\n",
      "0.416 0.01 0.001\n",
      "0.483 0.15 0.056\n",
      "0.483 0.15 0.1\n",
      "0.5 0.01 0.034\n",
      "0.5 0.01 0.045\n",
      "0.5 0.01 0.1\n",
      "0.5 0.15 0.023\n",
      "0.5 0.15 0.078\n",
      "0.01 0.15 0.023\n",
      "0.027 0.15 0.045\n",
      "0.027 0.15 0.056\n",
      "0.044 0.15 0.023\n",
      "0.044 0.15 0.034\n",
      "0.078 0.01 0.045\n",
      "0.078 0.01 0.056\n",
      "0.078 0.15 0.078\n",
      "0.094 0.01 0.001\n",
      "0.162 0.01 0.056\n",
      "0.162 0.15 0.001\n",
      "0.263 0.15 0.034\n",
      "0.263 0.15 0.056\n",
      "0.263 0.15 0.078\n",
      "0.263 0.15 0.1\n",
      "0.28 0.01 0.023\n",
      "0.28 0.01 0.056\n",
      "0.28 0.01 0.078\n",
      "0.28 0.15 0.023\n",
      "0.28 0.15 0.056\n",
      "0.28 0.15 0.089\n",
      "0.28 0.15 0.1\n",
      "0.297 0.01 0.034\n",
      "0.297 0.01 0.056\n",
      "0.297 0.01 0.078\n",
      "0.297 0.15 0.001\n",
      "0.399 0.01 0.034\n",
      "0.399 0.01 0.056\n",
      "0.399 0.01 0.089\n",
      "0.399 0.15 0.012\n",
      "0.416 0.01 0.023\n",
      "0.416 0.01 0.034\n",
      "0.416 0.01 0.045\n",
      "0.416 0.01 0.056\n",
      "0.416 0.01 0.078\n",
      "0.416 0.01 0.1\n",
      "0.416 0.15 0.012\n",
      "0.416 0.15 0.034\n",
      "0.416 0.15 0.056\n",
      "0.416 0.15 0.089\n",
      "0.432 0.01 0.001\n",
      "0.01 0.01 0.012\n",
      "0.044 0.15 0.067\n",
      "0.044 0.15 0.078\n",
      "0.061 0.15 0.023\n",
      "0.061 0.15 0.034\n",
      "0.078 0.15 0.045\n",
      "0.078 0.15 0.056\n",
      "0.094 0.01 0.034\n",
      "0.094 0.01 0.067\n",
      "0.094 0.15 0.001\n",
      "0.145 0.15 0.056\n",
      "0.145 0.15 0.089\n",
      "0.162 0.01 0.045\n",
      "0.162 0.01 0.078\n",
      "0.162 0.15 0.023\n",
      "0.162 0.15 0.1\n",
      "0.179 0.01 0.034\n",
      "0.179 0.01 0.089\n",
      "0.179 0.15 0.034\n",
      "0.179 0.15 0.078\n",
      "0.196 0.01 0.001\n",
      "0.348 0.01 0.067\n",
      "0.348 0.01 0.1\n",
      "0.348 0.15 0.023\n",
      "0.348 0.15 0.056\n",
      "0.348 0.15 0.089\n",
      "0.365 0.01 0.001\n",
      "0.432 0.15 0.012\n",
      "0.449 0.01 0.023\n",
      "0.449 0.01 0.045\n",
      "0.449 0.01 0.056\n",
      "0.449 0.01 0.1\n",
      "0.449 0.15 0.023\n",
      "0.449 0.15 0.056\n",
      "0.449 0.15 0.067\n",
      "0.449 0.15 0.089\n",
      "0.466 0.01 0.012\n",
      "0.466 0.01 0.089\n",
      "0.466 0.15 0.012\n",
      "0.483 0.01 0.023\n",
      "0.483 0.01 0.034\n",
      "0.483 0.01 0.056\n",
      "0.483 0.01 0.067\n",
      "0.483 0.01 0.078\n",
      "0.483 0.01 0.089\n",
      "0.483 0.15 0.012\n",
      "0.5 0.01 0.056\n",
      "0.5 0.01 0.089\n",
      "0.5 0.15 0.034\n",
      "0.5 0.15 0.056\n",
      "0.5 0.15 0.089\n",
      "0.01 0.15 0.056\n",
      "0.01 0.15 0.078\n",
      "0.01 0.15 0.089\n",
      "0.01 0.15 0.1\n",
      "0.027 0.01 0.012\n",
      "0.027 0.01 0.023\n",
      "0.027 0.01 0.034\n",
      "0.027 0.01 0.045\n",
      "0.027 0.01 0.056\n",
      "0.027 0.01 0.067\n",
      "0.027 0.01 0.078\n",
      "0.027 0.01 0.089\n",
      "0.027 0.01 0.1\n",
      "0.027 0.15 0.001\n",
      "0.145 0.01 0.056\n",
      "0.145 0.01 0.078\n",
      "0.145 0.15 0.045\n",
      "0.145 0.15 0.078\n",
      "0.162 0.01 0.012\n",
      "0.179 0.01 0.045\n",
      "0.179 0.01 0.078\n",
      "0.179 0.15 0.023\n",
      "0.196 0.01 0.012\n",
      "0.213 0.01 0.056\n",
      "0.213 0.15 0.001\n",
      "0.314 0.01 0.045\n",
      "0.314 0.01 0.056\n",
      "0.314 0.01 0.1\n",
      "0.314 0.15 0.023\n",
      "0.314 0.15 0.045\n",
      "0.314 0.15 0.067\n",
      "0.314 0.15 0.089\n",
      "0.331 0.01 0.012\n",
      "0.331 0.01 0.056\n",
      "0.331 0.01 0.089\n",
      "0.331 0.15 0.012\n",
      "0.348 0.01 0.034\n",
      "0.348 0.01 0.056\n",
      "0.348 0.01 0.078\n",
      "0.348 0.15 0.034\n",
      "0.348 0.15 0.045\n",
      "0.348 0.15 0.067\n",
      "0.348 0.15 0.078\n",
      "0.348 0.15 0.1\n",
      "0.365 0.01 0.012\n",
      "0.365 0.01 0.056\n",
      "0.365 0.01 0.1\n",
      "0.365 0.15 0.023\n",
      "0.365 0.15 0.034\n",
      "0.365 0.15 0.056\n",
      "0.382 0.01 0.034\n",
      "0.382 0.01 0.056\n",
      "0.382 0.01 0.067\n",
      "0.382 0.01 0.1\n",
      "0.382 0.15 0.023\n",
      "0.382 0.15 0.056\n",
      "0.382 0.15 0.078\n",
      "0.382 0.15 0.089\n",
      "0.399 0.01 0.001\n",
      "0.483 0.15 0.089\n",
      "0.5 0.01 0.023\n",
      "0.5 0.01 0.078\n",
      "0.5 0.15 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_103008/2801781346.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[0malgo_class\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBatchThompsonMixed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 107\u001B[0;31m results_all =  Parallel(n_jobs=-1)(\n\u001B[0m\u001B[1;32m    108\u001B[0m                              \u001B[0mdelayed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthompson_results\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0malgo_class\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m                              \u001B[0;32mfor\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrow\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult_experiments_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miterrows\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Appbooster/proba.ai/venv/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1052\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1053\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1054\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1055\u001B[0m             \u001B[0;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Appbooster/proba.ai/venv/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    931\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    932\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'supports_timeout'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 933\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    934\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Appbooster/proba.ai/venv/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mwrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    540\u001B[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001B[1;32m    541\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 542\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    543\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mCfTimeoutError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    544\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.9/concurrent/futures/_base.py\u001B[0m in \u001B[0;36mresult\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    438\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__get_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 440\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_condition\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    441\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    442\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_state\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mCANCELLED\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCANCELLED_AND_NOTIFIED\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.9/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m    \u001B[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    311\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 312\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    313\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    314\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def thompson_results(algo_class, *index):\n",
    "    p1_control, mde_test_effect, batch_size_share = index\n",
    "    p_list = [p1_control, p1_control * (1 + mde_test_effect)]\n",
    "    bts = algo_class(p_list_mu=p_list, batch_size_share_mu=batch_size_share)\n",
    "    probability_superiority_steps, expected_loss_step_list, \\\n",
    "    observations_step_list, k_list_iter = bts.start_experiment()\n",
    "    cumulative_observations_step_list = np.cumsum(observations_step_list, axis=0)\n",
    "    print(p1_control, mde_test_effect, batch_size_share)\n",
    "    return (probability_superiority_steps, expected_loss_step_list,\n",
    "            cumulative_observations_step_list, bts.n_obs_every_arm, k_list_iter\n",
    "            )\n",
    "\n",
    "def plot_mab_results(directory, name, result_experiments_df):\n",
    "    \"\"\"\n",
    "    Function creates two lines in one plot: left axis - probability superiority, right - cummulative observation\n",
    "    :param p_list_mu: expectation for conversion rates\n",
    "    :param batch_size_share_mu: expectation for batch size\n",
    "    :param probability_superiority_steps:\n",
    "    :param cumulative_observations_step_list:\n",
    "    :param folder:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    plot_file = PdfPages(f\"{directory}/{name}.pdf\")\n",
    "    for index, row in result_experiments_df.iterrows():\n",
    "        p1_control, mde_test_effect, batch_size_share = index\n",
    "        plt.figure(figsize=(15, 7));\n",
    "        x = np.arange(1, row['probability_superiority_steps'].shape[0]+1)\n",
    "        fig, ax1 = plt.subplots();\n",
    "\n",
    "        ax2 = ax1.twinx();\n",
    "        ax1.plot(x, row['cumulative_observations_step_list'][:, 0], '--', color='b', label='data1');\n",
    "        ax1.plot(x, row['cumulative_observations_step_list'][:, 1], '--', color='r', label='data2');\n",
    "        ax2.plot(x, np.array(row['probability_superiority_steps'])[:, 1], '-', color='r');\n",
    "        ax2.plot(x, row['expected_loss_steps'], ':', color='r')\n",
    "        plt.title(f\"p_list_mu: {[p1_control, p1_control * (1 + mde_test_effect)]} \\n\"\n",
    "                  f\"batch_size_share_mu, %: {batch_size_share * 100} \\n \"\n",
    "                  f\"dash lines - observations; red line - probability to win for 2 variant\",\n",
    "                  fontdict={\"size\": 5});\n",
    "        plot_file.savefig();\n",
    "        plt.close();\n",
    "    plot_file.close();\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "p1_control = np.round(np.linspace(0.01, 0.5, 30), 3)\n",
    "mde_test_effect = np.round(np.linspace(0.01, 0.15, 2), 3)\n",
    "batch_size_share = np.round(np.linspace(0.001, 0.1, 10), 3)\n",
    "\n",
    "result_experiments_df = pd.DataFrame(index=pd.MultiIndex.from_product(\n",
    "                                     [p1_control, mde_test_effect, batch_size_share],\n",
    "                                     names=[\"p1\", \"mde\", \"batch_size_share\"]),\n",
    "                                     columns=['probability_superiority_steps',\n",
    "                                              'expected_loss_steps',\n",
    "                                              'cumulative_observations_step_list',\n",
    "                                              'n_obs_per_every_arm',\n",
    "                                              'k_list_iter'])\n",
    "\n",
    "algo_class = BatchThompsonMixed\n",
    "results_all =  Parallel(n_jobs=-1)(\n",
    "                             delayed(thompson_results)(algo_class, *index)\n",
    "                             for index, row in tqdm(result_experiments_df.iterrows())\n",
    "                             )\n",
    "i = 0\n",
    "for index, row in  result_experiments_df.iterrows():\n",
    "    result_experiments_df.loc[index, \"probability_superiority_steps\"] = results_all[i][0]\n",
    "    result_experiments_df.loc[index, \"expected_loss_steps\"] = results_all[i][1]\n",
    "    result_experiments_df.loc[index, \"cumulative_observations_step_list\"] = results_all[i][2]\n",
    "    result_experiments_df.loc[index, \"n_obs_per_every_arm\"] = results_all[i][3]\n",
    "    result_experiments_df.loc[index, \"k_list_iter\"] = results_all[i][4]\n",
    "    i += 1\n",
    "plot_mab_results(\"Plot/Thompson/Experiment4\", \"ThompsonMixed_new\",\n",
    "                 result_experiments_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "25897"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_size_zratio(0.5, 0.5 * (1 + 0.01), alpha=0.05, beta=0.2)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}